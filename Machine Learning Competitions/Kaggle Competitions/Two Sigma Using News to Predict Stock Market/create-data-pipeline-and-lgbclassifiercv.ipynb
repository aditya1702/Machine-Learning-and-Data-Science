{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nfrom datetime import datetime, timedelta, date\nimport numpy as np\nimport pandas as pd\nfrom numpy import nanmean\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom multiprocessing import Pool, cpu_count\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom kaggle.competitions import twosigmanews\nimport lightgbm as lgb\nfrom joblib import Parallel, delayed\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss, make_scorer\n\npd.set_option('max_columns', 50)\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"env = twosigmanews.make_env()\nmarket_train, news_train = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2492e1410661dd1ab32d5db4c4c7f5eb1166a709"},"cell_type":"code","source":"del news_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1352743e69e6e2b33b67e508c5c4dad4fb7eb5e8"},"cell_type":"markdown","source":"## Initial Preprocessing"},{"metadata":{"trusted":true,"_uuid":"f77edafed22a511e82475160ad12b1c5359b3277"},"cell_type":"code","source":"class InitialPreprocessing(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n\n    def fit(self, X, y = None):\n        print(\"Initial Preprocessing\")\n        return self\n    \n    def transform(self, X, y = None):\n        \n        # We will work on data after 2010\n        if len(X) == 1:\n            X[0]['date'] = X[0]['time'].dt.date\n            X[0] = X[0].loc[X[0]['date'] >= date(2010, 1, 1)]\n        else:\n            X[0]['date'] = X[0]['time'].dt.date\n            X[1]['date'] = X[1]['time'].dt.date\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53e23d13a202bfcdbc89fd3a4dc5bd8411b7474f"},"cell_type":"markdown","source":"## Adding Quant Features"},{"metadata":{"trusted":true,"_uuid":"ba245a08b0d0b70b0bca6e3b63781ba1c9a67ade"},"cell_type":"code","source":"class AddingQuantFeatures(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.feature_cols = ['close', 'open', 'volume']\n        self.moving_average_number_of_days = [10, 30, 90]\n        self.exponential_moving_average_span = [10, 30, 90]\n        self.bollinger_band_number_of_days = [7]\n        self.ewma = pd.Series.ewm\n        self.no_of_std = 2\n        self.returns_features = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                            'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n        self.drop_features = ['time', 'assetName', 'volume', 'close', 'open',\n                               'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                               'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                               'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n                               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n        self.final_drop_features = []\n        \n    def _compute_moving_average(self, X):\n        for column in self.feature_cols:\n            for window_period in self.moving_average_number_of_days:\n                feature_column_name = 'moving_average_{0}_{1}_days'.format(column, window_period)\n                X[feature_column_name] = X.groupby('assetCode')[column].apply(lambda x: x.rolling(window_period).mean())\n        return X\n    \n    def _compute_exponential_moving_average(self, X):\n        for column in self.feature_cols:\n            for span_period in self.exponential_moving_average_span:\n                feature_column_name = 'exponential_moving_average_{0}_{1}_days'.format(column, span_period)\n                X[feature_column_name] = X.groupby('assetCode')[column].apply(lambda x: self.ewma(x, span = span_period).mean())\n        return X\n    \n    def _compute_macd(self, X):\n        for column in self.feature_cols[1:3]:\n            feature_column_name = 'macd_{0}'.format(column)\n            X[feature_column_name] = (X.groupby('assetCode')[column].apply(lambda x: self.ewma(x, span = 12).mean()) - \n                                     X.groupby('assetCode')[column].apply(lambda x: self.ewma(x, span = 26).mean()))\n        return X\n    \n    def _compute_bollinger_band(self, X):\n        \n        # We will not take volume into account here\n        for column in self.feature_cols[1:3]:\n            for window_period in self.bollinger_band_number_of_days:\n                bb_high_feature_column_name = 'bollinger_band_{0}_{1}_days_high'.format(column, window_period)\n                bb_low_feature_column_name = 'bollinger_band_{0}_{1}_days_low'.format(column, window_period)\n                std_feature_column_name = 'moving_average_std_{0}_{1}_days'.format(column, window_period)\n                moving_average_feature_column_name = 'moving_average_{0}_{1}_days'.format(column, window_period)\n                \n                X[std_feature_column_name] = X.groupby('assetCode')[column].apply(lambda x: x.rolling(window_period).std())\n                X[bb_high_feature_column_name] = X[moving_average_feature_column_name] + self.no_of_std * X[std_feature_column_name]\n                X[bb_low_feature_column_name] = X[moving_average_feature_column_name] - self.no_of_std * X[std_feature_column_name]\n        return X\n    \n    def _compute_rsi(self, X):\n        return\n    \n    def _compute_miscellaneous_features(self, X):\n        for column in self.returns_features:\n            feature_column_name = '{0}_average'.format(column)\n            mean_by_date = X.groupby('date')[column].agg({feature_column_name: nanmean}).reset_index()\n            X = pd.merge(X, mean_by_date, how = 'left', on = ['date'])\n            \n            \n        X['close_open_ratio'] = X['close']/X['open']\n        mean_by_date = X.groupby('date')['close_open_ratio'].agg({'close_open_ratio_average': nanmean}).reset_index()\n        X = pd.merge(X, mean_by_date, how = 'left', on = ['date'])\n        return X\n    \n    def fit(self, X, y = None):\n        print(\"Adding Quant Features\")\n        self.quant_features_functions = [#self._compute_moving_average, \n                                         #self._compute_exponential_moving_average, \n                                         self._compute_miscellaneous_features\n                                        ]\n        return self\n    \n    def transform(self, X, y = None):\n\n        # Apply each quant function to X\n        for quant_function in self.quant_features_functions:\n            X[0] = quant_function(X[0])\n        \n        if len(X) == 1:\n            X = X[0]\n        else:\n            X[0] = X[0].drop(self.drop_features, 1)\n            X = pd.merge(X[1], X[0], how = 'left', on = ['date', 'assetCode'])\n        \n        gc.collect()\n        return X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d75e7e61ec21571fca9d7cb123d37bb1b58045e"},"cell_type":"markdown","source":"## Adding Lag Features"},{"metadata":{"_uuid":"0bb3376a6218043b20c1e2ea01edd82c90e5a7fe","trusted":true},"cell_type":"code","source":"class AddingLagFeatures(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.shift_size = 1\n        self.n_lag = [3, 7, 14]\n        self.return_features = ['returnsClosePrevMktres10', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'open', 'close']\n        self.drop_features = [ 'time', 'assetName', 'volume', 'close', 'open',\n                               'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                               'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                               'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n                               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n        \n    def fit(self, X, y = None):\n        print(\"Adding Lag Features\")\n        return self\n    \n    def _create_lag_features(self, X):\n        for col in self.return_features:\n            for window in self.n_lag:\n                rolled = X[col].shift(self.shift_size).rolling(window = window)\n                lag_mean = rolled.mean()\n                lag_max = rolled.max()\n                lag_min = rolled.min()\n                X['%s_lag_%s_mean'%(col, window)] = lag_mean\n                X['%s_lag_%s_max'%(col, window)] = lag_max\n                X['%s_lag_%s_min'%(col, window)] = lag_min\n        gc.collect()\n        \n        return X.fillna(-1)\n    \n    def transform(self, X, y = None):\n        assetCodes = X[0]['assetCode'].unique()\n        \n        all_df = []\n        df_codes = X[0].groupby('assetCode')\n        df_codes = [df_code[1][['date', 'assetCode'] + self.return_features] for df_code in df_codes]\n\n        pool = Pool(4)\n        all_df = pool.map(self._create_lag_features, df_codes)\n        new_df = pd.concat(all_df)\n        new_df.drop(self.return_features, axis = 1, inplace = True)\n        X[0] = pd.merge(X[0], new_df, how = 'left', on = ['date', 'assetCode'])\n        \n        if len(X) == 1:\n            X = X[0]\n        else:\n            X[0] = X[0].drop(self.drop_features, 1)\n            X = pd.merge(X[1], X[0], how = 'left', on = ['date', 'assetCode'])\n        pool.close()\n        \n        del all_df, new_df, df_codes, assetCodes\n        gc.collect()\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1910e076fa2559b4396c30380062fe612120a975"},"cell_type":"markdown","source":"## Impute Missing Values"},{"metadata":{"trusted":true,"_uuid":"b7c11392fc49d927cdbd5018f7d4a91a92f233a7"},"cell_type":"code","source":"class MissingValuesImputer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n        \n    def fit(self, X, y = None):\n        print(\"Missing Values Imputer\")\n        return self\n    \n    def transform(self, X, y = None):\n        \n        for i in X.columns:\n            if X[i].dtype == \"object\":\n                X[i] = X[i].fillna(\"other\")\n            elif (X[i].dtype == \"int64\" or X[i].dtype == \"float64\"):\n                X[i] = X[i].fillna(X[i].mean())\n            else:\n                pass\n            \n        return X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35b64d0a838dbf43650b79ce3237afba966f350d"},"cell_type":"markdown","source":"## Encoding AssetCode"},{"metadata":{"trusted":true,"_uuid":"84be08fb7a8c3a7cd5d5ca50c3cf64f9055bee27"},"cell_type":"code","source":"class AssetcodeEncoder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n        \n    def fit(self, X, y = None):\n        print(\"Encoding AssetCode\")\n        return self\n    \n    def transform(self, X, y = None):\n        lbl = {k: v for v, k in enumerate(X['assetCode'].unique())}\n        X['assetCodeT'] = X['assetCode'].map(lbl)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6901f17fac39775db78edca4ffdb01967e24500"},"cell_type":"markdown","source":"<h2>3. Model Building </h2>"},{"metadata":{"trusted":true,"_uuid":"830d46f30c1a510066318e7616fd27640cc1386b"},"cell_type":"code","source":"class LGBClassifierCV(BaseEstimator, RegressorMixin):\n    \n    def __init__(self, \n                 axis = 0, \n                 lgb_params = None, \n                 fit_params = None, \n                 cv = 5, \n                 perform_bayes_search = False, \n                 perform_random_search = False, \n                 use_train_test_split = False,\n                 use_full_data = True,\n                 use_kfold_split = True):\n        self.axis = axis\n        self.lgb_params = lgb_params\n        self.fit_params = fit_params\n        self.fit_params = fit_params\n        self.cv = cv\n        self.perform_random_search = perform_random_search\n        self.perform_bayes_search = perform_bayes_search\n        self.use_train_test_split = use_train_test_split\n        self.use_kfold_split = use_kfold_split\n        self.use_full_data = use_full_data\n    \n    def _get_random_search_params(self, X, y):\n        # Define a search space for the parameters\n        lgb_search_params = {\n            'learning_rate': [0.15, 0.1, 0.05, 0.02, 0.01],\n            'num_leaves': [i for i in range(12, 90, 6)],\n            'n_estimators': [100, 200, 300, 400, 500, 600, 800],\n            'min_child_samples': [i for i in range(10, 100, 10)],\n            'colsample_bytree': [0.8, 0.9, 0.95, 1],\n            'subsample': [0.8, 0.9, 0.95, 1],\n            'reg_alpha': [0.1, 0.2, 0.4, 0.6, 0.8],\n            'reg_lambda': [0.1, 0.2, 0.4, 0.6, 0.8],\n        }\n\n        x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, shuffle = False)\n        best_eval_score = 0\n        for i in range(100):\n            print(\"random search iteration - \" + str(i+1))\n            params = {k: np.random.choice(v) for k, v in lgb_search_params.items()}\n            \n            # Use 2 cores/threads\n            params['n_jobs'] = cpu_count() - 1\n            \n            score = self._evaluate_model(x_train, x_val, y_train, y_val, params)\n            if score < best_eval_score or best_eval_score == 0:\n                best_eval_score = score\n                best_params = params\n        print(\"Best evaluation logloss\", best_eval_score)\n        print(best_params)\n        \n        return best_params\n            \n    def _custom_metric(self, dates, pred_proba, num_target, universe):\n        y = 2*pred_proba[:, 1] - 1\n\n        # get rid of outliers\n        r = num_target.clip(-1,1)\n        x = y * r * universe\n        result = pd.DataFrame({'day' : dates, 'x' : x})\n        x_t = result.groupby('day').sum().values\n        return np.mean(x_t) / np.std(x_t)\n    \n    def _evaluate_model(self, x_train, x_val, y_train, y_val, params):\n        model = LGBMClassifier(**params)\n        model.fit(x_train, y_train)\n        return log_loss(y_val, model.predict_proba(x_val))\n    \n    def find_best_params_(self, X, y):\n        if self.perform_random_search:\n            self.lgb_optimal_params = self._get_random_search_params(X, y)\n    \n    def fit(self, X, y = None, **fit_params):\n        print(\"LGBClassifierCV\")\n        \n        # Segregate the target columns\n        dates = X['date']\n        num_target = X['returnsOpenNextMktres10'].astype('float32')\n        y = (X['returnsOpenNextMktres10'] >= 0).astype('int8')\n        universe = X['universe'].astype('int8')\n\n        # Drop columns that are not features\n        X.drop(['returnsOpenNextMktres10', 'universe', 'assetCode', 'assetName', 'time', 'date'], axis = 1, inplace = True)\n        print(X.columns)\n        \n        # Scaling the X values\n        self.mins = np.min(X, axis = 0)\n        self.maxs = np.max(X, axis = 0)\n        self.rng = self.maxs - self.mins\n        X = 1 - ((self.maxs - X) /self.rng)\n        gc.collect()\n        \n         # Find the best params using random search or random search\n        if self.perform_random_search:\n            self.find_best_params_(X, y)\n        else:\n            self.lgb_optimal_params = {'learning_rate': 0.01, \n                                       'num_leaves': 66, \n                                       'n_estimators': 200, \n                                       'min_child_samples': 40, \n                                       'colsample_bytree': 0.9, \n                                       'subsample': 0.9, \n                                       'reg_alpha': 0.4, \n                                       'reg_lambda': 0.2, \n                                       'n_jobs': 3}\n            \n            x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n            x_2 = [0.19016805202090095, 2583, 213, 312, 220]\n            \n            self.lgb_optimal_params_1 = {\n                            'task': 'train',\n                            'boosting_type': 'gbdt',\n                            'objective': 'binary',\n                            'learning_rate': x_1[0],\n                            'num_leaves': x_1[1],\n                            'min_data_in_leaf': x_1[2],\n                            'num_iteration': 239,\n                            'max_bin': x_1[4],\n                            'verbose': 1\n                        }\n\n            self.lgb_optimal_params_2 = {\n                            'task': 'train',\n                            'boosting_type': 'gbdt',\n                            'objective': 'binary',\n                            'learning_rate': x_2[0],\n                            'num_leaves': x_2[1],\n                            'min_data_in_leaf': x_2[2],\n                            'num_iteration': 172,\n                            'max_bin': x_2[4],\n                            'verbose': 1\n                        }\n            \n        self.lgb_optimal_params[\"objective\"] = \"binary\"\n        self.lgb_optimal_params[\"boosting_type\"] = \"gbdt\"\n        \n        # Train on full dataset\n        if self.use_full_data:\n            train_data = lgb.Dataset(X, label = y)\n            self.lgb_model_1 = lgb.train(self.lgb_optimal_params_1, train_set = train_data, num_boost_round = 100)\n            self.lgb_model_2 = lgb.train(self.lgb_optimal_params_2, train_set = train_data, num_boost_round = 100)\n        \n        # Use a simple train-test split\n        if self.use_train_test_split:\n            x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, shuffle = False)\n            \n            train_data = lgb.Dataset(x_train, label = y_train)\n            val_data = lgb.Dataset(x_val, label = y_val)\n            \n            self.lgb_model = lgb.train(self.lgb_optimal_params,\n                                       train_set = train_data, \n                                       num_boost_round = 100,\n                                       valid_sets = [val_data], \n                                       early_stopping_rounds = 5)\n            \n        # When not using random search to tune parameters, proceed with a simple Stratified Kfold CV\n        if self.use_kfold_split:\n            kf = StratifiedKFold(n_splits = self.cv, shuffle = False)\n            for fold_index, (train_data, val_data) in enumerate(kf.split(X, y)):\n                print(\"Train Fold Index - \" + str(fold_index))\n\n                lgb_model = lgb.LGBMClassifier(**self.lgb_optimal_params)\n                lgb_model = lgb.train(self.lgb_optimal_params,\n                                       train_set = train_data, \n                                       num_boost_round = 100,\n                                       valid_sets = [val_data], \n                                       early_stopping_rounds = 5)\n                self.estimators_.append(lgb_model)\n        return self\n    \n    def predict(self, X):\n        # Drop columns that are not features\n        X.drop(['assetCode', 'assetName', 'time', 'date'], axis = 1, inplace = True)\n        \n        # Scale the values\n        X = 1 - ((self.maxs - X) /self.rng)\n\n        # Get the predictions    \n        predictions = (self.lgb_model_1.predict(X) + self.lgb_model_2.predict(X))/2\n        predictions = (predictions - predictions.min())/(predictions.max() - predictions.min())\n        predictions = predictions * 2 - 1\n    \n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73245fe6f2f1c0f87ccb95ae47b8c453eb8d5711"},"cell_type":"markdown","source":"<h2>4. Create a Pipeline</h2>"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5b123ba733afa1edc9ea01a2cd9c3a9d19ac4ebd"},"cell_type":"code","source":"data_pipeline = Pipeline([\n    ('initial_preprocessor', InitialPreprocessing()),\n    ('quant_features_generator', AddingQuantFeatures()),\n#     ('lag_features_generator', AddingLagFeatures()),\n    ('data_imputer', MissingValuesImputer()),\n    ('asset_encoder', AssetcodeEncoder()),\n    ('lgb', LGBClassifierCV(cv = 5,\n                            perform_random_search = False,\n                            use_train_test_split = False,\n                            use_full_data = True,\n                            use_kfold_split = False)\n    )\n])\n\ndata_pipeline.fit([market_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7d526fa6b040a3f8b69c0f532b36d3bd75b491d"},"cell_type":"code","source":"def write_submission(pipeline, env):\n    days = env.get_prediction_days()\n    n_days = 0\n    total_market_obs = []\n    for (market_obs_df, news_obs_df, predictions_template_df) in days:\n        n_days += 1\n        print(n_days,end = ' ')\n        \n#         total_market_obs.append(market_obs_df.copy())\n#         if len(total_market_obs) == 1:\n#             history_df = total_market_obs[0]\n#         else:\n#             history_df = pd.concat(total_market_obs[-(14+1):], ignore_index = True)\n        \n        preds = data_pipeline.predict([market_obs_df])\n        sub = pd.DataFrame({'assetCode': market_obs_df['assetCode'], 'confidence': preds})\n        predictions_template_df = predictions_template_df.merge(sub, how = 'left').drop(\n            'confidenceValue', axis = 1).fillna(0).rename(columns = {'confidence': 'confidenceValue'})\n        \n        env.predict(predictions_template_df)\n        del predictions_template_df, preds, sub\n        gc.collect()\n    env.write_submission_file()\n    \nwrite_submission(data_pipeline, env)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a3010f47cfc9db39abd6ba064273b6214a0bb7e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}